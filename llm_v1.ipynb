{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83b3dc97-0f8f-4016-8888-42803a967aba",
   "metadata": {},
   "source": [
    "# Mini Türkçe Transformer Modeli - V1\n",
    "\n",
    "Bu çalışma sıfırdan yazılmış basit bir Transformer Decoder modelidir.\n",
    "\n",
    "Amaç:\n",
    "- Tokenizer mantığını anlamak\n",
    "- Embedding ve Attention yapısını görmek\n",
    "- Küçük veri üzerinde dil modeli eğitmek\n",
    "\n",
    "Not:\n",
    "Bu model eğitim amaçlıdır, üretim için optimize edilmemiştir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4b18434-bbe2-49ba-9a01-7572ae72890b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4757e8ae-367b-41f5-88d8-3f875d6fc38f",
   "metadata": {},
   "source": [
    "## VOCAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd479ca8-81b7-46cf-b861-cda86b75a7bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 163\n",
      "{'.': 0, '2014': 1, '2015': 2, '8': 3, 'ait': 4, 'ana': 5, 'ancak': 6, 'arama': 7, 'araştırmacılar': 8, 'artırdı': 9, 'arıza': 10, 'askeri': 11, 'attı': 12, 'avustralya': 13, 'açtı': 14, 'batıya': 15, 'bazı': 16, 'belirlenemedi': 17, 'bilinçli': 18, 'bir': 19, 'biri': 20, 'birçok': 21, 'bu': 22, 'bulunamadı': 23, 'bulundu': 24, 'bulunuyordu': 25, 'büyük': 26, 'da': 27, 'daha': 28, 'deniz': 29, 'değiştirdi': 30, 'dokuz': 31, 'doğru': 32, 'doğruladı': 33, 'durdu': 34, 'durum': 35, 'düzenlemelere': 36, 'edilmektedir': 37, 'ekipleri': 38, 'en': 39, 'enkaz': 40, 'etti': 41, 'farklı': 42, 'geliştirilmesi': 43, 'geniş': 44, 'gerektiği': 45, 'gizemini': 46, 'gizemlerinden': 47, 'göre': 48, 'gösterdi': 49, 'gövdesi': 50, 'güvenliği': 51, 'hala': 52, 'hava': 53, 'havacılık': 54, 'havada': 55, 'havayollarına': 56, 'hint': 57, 'iddia': 58, 'ihtimali': 59, 'iki': 60, 'ile': 61, 'iletişimini': 62, 'ise': 63, 'kabul': 64, 'kaldığını': 65, 'kalkıştan': 66, 'kanat': 67, 'kapandı': 68, 'kara': 69, 'katıldı': 70, 'kaybetti': 71, 'kayboldu': 72, 'kesin': 73, 'kişiler': 74, 'kontrolü': 75, 'konumu': 76, 'konusunda': 77, 'kuala': 78, 'kutusu': 79, 'kısa': 80, 'kıyılarında': 81, 'lumpur': 82, 'madagaskar': 83, 'malezya': 84, 'mart': 85, 'mh370': 86, 'modern': 87, 'mozambik': 88, 'müdahalesi': 89, 'mürettebat': 90, 'okyanusu': 91, 'okyanusunda': 92, 'olabileceğini': 93, 'olarak': 94, 'olay': 95, 'olayın': 96, 'olduğunu': 97, 'operasyonlar': 98, 'ortaya': 99, 'otuz': 100, 'parçaları': 101, 'parçanın': 102, 'parçası': 103, 'pekin': 104, 'pilot': 105, 'radar': 106, 'radarlardan': 107, 'resmi': 108, 'rotasını': 109, 'saatlerce': 110, 'sayılı': 111, 'sefer': 112, 'sistemi': 113, 'sistemleri': 114, 'sistemlerinin': 115, 'sonar': 116, 'sonra': 117, 'sonuca': 118, 'soruşturmalar': 119, 'süre': 120, 'tabanı': 121, 'takip': 122, 'tarandı': 123, 'tarihinde': 124, 'tarihinin': 125, 'teknik': 126, 'teoriler': 127, 'teorilere': 128, 'trafik': 129, 'transponder': 130, 'ulaşamadı': 131, 'uydu': 132, 'uzmanlar': 133, 'uçak': 134, 'uçakta': 135, 'uçağı': 136, 'uçağın': 137, 'uçağına': 138, 'uçuyordu': 139, 'vakası': 140, 've': 141, 'verileri': 142, 'verilerine': 143, 'vurgulandı': 144, 'yeni': 145, 'yol': 146, 'yolcu': 147, 'yoğunlaştı': 148, 'yöneldi': 149, 'yürüttü': 150, 'yüz': 151, 'yılında': 152, 'çalışmaları': 153, 'çalışmalarına': 154, 'çaplı': 155, 'çin': 156, 'ülke': 157, 'üzerinde': 158, 'şehrinden': 159, 'şehrine': 160, ' ': 161, '<unk>': 162}\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "malezya havayollarına ait mh370 sefer sayılı yolcu uçağı 8 mart 2014 tarihinde kayboldu.\n",
    "uçak kuala lumpur şehrinden pekin şehrine doğru uçuyordu.\n",
    "uçakta iki yüz otuz dokuz yolcu ve mürettebat bulunuyordu.\n",
    "\n",
    "uçak kalkıştan kısa bir süre sonra hava trafik kontrolü ile iletişimini kaybetti.\n",
    "transponder sistemi kapandı ve uçak radarlardan kayboldu.\n",
    "askeri radar verilerine göre uçak rotasını değiştirdi ve batıya doğru yöneldi.\n",
    "\n",
    "uydu verileri uçağın saatlerce havada kaldığını gösterdi.\n",
    "ancak kesin konumu belirlenemedi.\n",
    "arama çalışmaları hint okyanusu üzerinde yoğunlaştı.\n",
    "\n",
    "birçok ülke arama çalışmalarına katıldı.\n",
    "avustralya çin ve malezya ekipleri geniş çaplı operasyonlar yürüttü.\n",
    "deniz tabanı sonar sistemleri ile tarandı.\n",
    "\n",
    "2015 yılında hint okyanusunda bir kanat parçası bulundu.\n",
    "uzmanlar bu parçanın mh370 uçağına ait olduğunu doğruladı.\n",
    "daha sonra mozambik ve madagaskar kıyılarında da bazı enkaz parçaları bulundu.\n",
    "\n",
    "ancak uçağın ana gövdesi ve kara kutusu hala bulunamadı.\n",
    "bu durum olayın gizemini artırdı.\n",
    "uzmanlar farklı teoriler ortaya attı.\n",
    "\n",
    "bazı teorilere göre uçak bilinçli olarak rotasını değiştirdi.\n",
    "bazı araştırmacılar teknik bir arıza ihtimali üzerinde durdu.\n",
    "bazı kişiler ise pilot müdahalesi olabileceğini iddia etti.\n",
    "\n",
    "resmi soruşturmalar kesin bir sonuca ulaşamadı.\n",
    "olay modern havacılık tarihinin en büyük gizemlerinden biri olarak kabul edilmektedir.\n",
    "mh370 vakası havacılık güvenliği konusunda yeni düzenlemelere yol açtı.\n",
    "uçak takip sistemlerinin geliştirilmesi gerektiği vurgulandı.\n",
    "\"\"\"\n",
    "\n",
    "tokens_raw = text.replace(\".\", \" .\").split()\n",
    "unique_tokens = sorted(set(tokens_raw))\n",
    "\n",
    "vocab = {token: idx for idx, token in enumerate(unique_tokens)}\n",
    "\n",
    "# özel tokenlar\n",
    "vocab[\" \"] = len(vocab)\n",
    "vocab[\"<unk>\"] = len(vocab)\n",
    "\n",
    "print(\"Vocab size:\", len(vocab))\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771ba0fa-e258-41bd-bbc4-d7b0595b5b9b",
   "metadata": {},
   "source": [
    "## 1 - TOKENIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63f32d40-6d33-4e22-96f5-01618497e84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "  def __init__(self, vocab_json):\n",
    "    self.vocab = vocab_json\n",
    "    self.reverse_vocab = {v: k for k, v in self.vocab.items()}\n",
    "\n",
    "  def encode(self, text):\n",
    "    tokens = []\n",
    "    for word in text.split():\n",
    "      i = 0\n",
    "      while i < len(word):\n",
    "        found_match = False\n",
    "        for j in range(len(word), i, -1):\n",
    "          subword = word[i:j]\n",
    "          if subword in self.vocab:\n",
    "            tokens.append(self.vocab[subword])\n",
    "            found_match = True\n",
    "            i=j\n",
    "            break\n",
    "        if not found_match:\n",
    "          tokens.append(self.vocab['<unk>'])\n",
    "          i += 1\n",
    "      tokens.append(self.vocab[\" \"])\n",
    "    tokens.pop()\n",
    "    return tokens\n",
    "\n",
    "  def decode(self, ids):\n",
    "    text = \"\"\n",
    "    for id in ids:\n",
    "      text += self.reverse_vocab[id]\n",
    "    return text\n",
    "  \n",
    "  def tokenize(self, text):\n",
    "    ids = self.encode(text)\n",
    "    words = [self.reverse_vocab[id] for id in ids]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1194c3-a82d-42c5-b49d-0053d6c6187e",
   "metadata": {},
   "source": [
    "## EMBEDDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35256c33-88d1-4a0c-af9b-c00410a61ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rotary_position_encoding(input: torch.Tensor, base=10000, device=\"cpu\"):\n",
    "    context_length, embedding_dim = input.shape\n",
    "    assert embedding_dim % 2 == 0, \"Embedding dimension must be even for rotary position encoding.\"\n",
    "    half_dim = embedding_dim // 2\n",
    "    \n",
    "    freq_indices = torch.arange(half_dim, device=device, dtype=torch.float32)\n",
    "    freqs = 1.0 / (base ** (freq_indices / half_dim))\n",
    "    \n",
    "    positions = torch.arange(context_length, device=device, dtype=torch.float32).unsqueeze(1)\n",
    "    \n",
    "    angles = positions * freqs\n",
    "    sin_angles = torch.sin(angles)\n",
    "    cos_angles = torch.cos(angles)\n",
    "    \n",
    "    input_even = input[:, :half_dim]\n",
    "    input_odd = input[:, half_dim:]\n",
    "    \n",
    "    rotated_input_even = input_even * cos_angles - input_odd * sin_angles\n",
    "    rotated_input_odd = input_even * sin_angles + input_odd * cos_angles\n",
    "    \n",
    "    input_rotated = torch.zeros_like(input)\n",
    "    input_rotated[:, :half_dim] = rotated_input_even\n",
    "    input_rotated[:, half_dim:] = rotated_input_odd\n",
    "    \n",
    "    return input_rotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad2eb258-509f-49bf-8f20-74cc4b837b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.pos_embedding = get_rotary_position_encoding\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        x = self.embedding(input_ids) # dictory'deki kelimelerin vektör temsilleri\n",
    "        x = self.pos_embedding(x) # pozisyonlarına göre döndürülmüş vektör temsilleri\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3b66c4-f789-46ac-b10e-d73c5d0d4159",
   "metadata": {},
   "source": [
    "## MULTI-HEAD ATTENTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30320e70-8d74-41a2-b18d-5d935daad6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embedding_dim, output_dim, context_length, num_heads=2, dropout_rate=0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.context_length = context_length\n",
    "        self.multi_head_attention = nn.MultiheadAttention(embedding_dim, num_heads, dropout=dropout_rate)\n",
    "        self.projection = nn.Linear(embedding_dim, output_dim)\n",
    "        \n",
    "        self.register_buffer(\"mask\", torch.tril(torch.ones(context_length, context_length)))  # Maksimum context length için mask\n",
    "    \n",
    "    def forward(self, x):\n",
    "        number_of_tokens = x.shape[0]\n",
    "        x = x[:self.context_length]\n",
    "        x = self.multi_head_attention(x, x, x, attn_mask=self.mask[:number_of_tokens, :number_of_tokens])[0]\n",
    "        x = self.projection(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6287d62e-1561-4701-b53f-ea4f99eabe26",
   "metadata": {},
   "source": [
    "## LAYER NORMALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08afa06b-d922-450f-a594-b0b60d50fddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, embedding_dim, eps=1e-5):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(embedding_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        normalized = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.weight * normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382a6eea-4d97-4896-a8dc-4ec78745a404",
   "metadata": {},
   "source": [
    "## ACTIVATION FUNCTION - GELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25703a43-a787-4baf-8df4-6c32535c94a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(torch.sqrt(2 / torch.tensor(torch.pi)) * (x + 0.044715 * torch.pow(x, 3))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a151451e-aa17-40b6-b088-228ee2863cda",
   "metadata": {},
   "source": [
    "## MLP (MULTI-LAYER PERCEPTRON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19db567e-5f22-484c-9fb0-eb8be4900ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.gate_proj = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.up_proj = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.down_proj = nn.Linear(hidden_dim, embedding_dim)\n",
    "        \n",
    "        self.gelu = GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        gate = self.gate_proj(x)\n",
    "        gate = self.gelu(gate)\n",
    "        up = self.up_proj(x)\n",
    "        fuse = gate * up\n",
    "        outputs = self.down_proj(fuse)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895686f8-69af-4059-99a4-5a83a94472b4",
   "metadata": {},
   "source": [
    "## DECODER BLOCK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7144ca8-de4c-4144-94f0-a5d1e3f6b5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embedding_dim, context_length, num_heads=2, dropout_rate=0.5):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.self_attention = MultiHeadAttention(embedding_dim, embedding_dim, context_length, num_heads=num_heads, dropout_rate=dropout_rate)\n",
    "        self.norm1 = LayerNorm(embedding_dim)\n",
    "        self.mlp = MLP(embedding_dim, embedding_dim)\n",
    "        self.norm2 = LayerNorm(embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "          |------> Norm Block 1 (1) --------------------------------------------> |                                 |----------> Norm Block 2 ------------------------> |\n",
    "        x |                                                                       |----> + (First connection)-----> |                                                   |------> + (Second connection)\n",
    "          |------> Multi-head Attention Block (2) -----> Norm Block 1 (3) ------> |                                 |----------> MLP Block -----> Norm Block 2 -------> |\n",
    "        \"\"\"\n",
    "        # First Connection\n",
    "        normalized_x = self.norm1(x) # (1) Normalization\n",
    "        \n",
    "        attention_x = self.self_attention(x) # (2) Multi-head Attention\n",
    "        post_attention_x = self.norm1(attention_x) # (3) Normalization after attention\n",
    "        \n",
    "        x = normalized_x + post_attention_x # Residual connection after attention\n",
    "        \n",
    "        # Second Connection\n",
    "        normalized_x = self.norm2(x) \n",
    "        \n",
    "        mlp_x = self.mlp(x) \n",
    "        post_attention_x = self.norm2(mlp_x) \n",
    "        \n",
    "        x = normalized_x + post_attention_x \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9b6a40-d01f-4296-a9b8-071e202c6dcf",
   "metadata": {},
   "source": [
    "## 2- MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4291b083-39e9-435b-8b1b-02adcd39ff75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MasterLLMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, context_length, dropout_rate=0.5, num_heads=2, num_layers=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = Embedding(vocab_size=vocab_size, embedding_dim=embedding_dim)\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            *[DecoderBlock(embedding_dim, context_length, num_heads=num_heads, dropout_rate=dropout_rate) for _ in range(num_layers)]\n",
    "            )\n",
    "        \n",
    "        self.lm_head = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        x = self.embedding(input_ids) # dictory'deki kelimelerin vektör temsilleri\n",
    "        x = self.layers(x)\n",
    "        x = self.lm_head(x)\n",
    "        return x\n",
    "    \n",
    "    def generate(self, x, max_new_tokens):\n",
    "        tokens = x\n",
    "        x = torch.tensor(x)\n",
    "        \n",
    "        for _ in range(max_new_tokens):\n",
    "            out = self.forward(x)\n",
    "            probs = torch.softmax(out[-1, :], dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            tokens.append(next_token.item())\n",
    "            \n",
    "            if next_token == 59 or len(tokens) > 32: # eos and max context length\n",
    "                break\n",
    "            \n",
    "            x = torch.tensor(tokens)\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2be65e-bbbb-4f0c-9ae6-6115266df09b",
   "metadata": {},
   "source": [
    "## TEXT DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3073cc88-733a-4442-8bbb-78e29ad26a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_id = vocab[\" \"]\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, token_ids:list, context_length:int, stride:int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.inputs = []\n",
    "        self.targets = []\n",
    "\n",
    "        for i in range(0, len(token_ids) - context_length, stride):\n",
    "            input_chunk = token_ids[i : i+context_length]\n",
    "            target_chunk = token_ids[i+1 : i+context_length+1]\n",
    "\n",
    "            # padding\n",
    "            input_chunk = input_chunk + [pad_id] * (context_length - len(input_chunk))\n",
    "            target_chunk = target_chunk + [pad_id] * (context_length - len(target_chunk))\n",
    "\n",
    "            # truncate\n",
    "            input_chunk = input_chunk[:context_length]\n",
    "            target_chunk = target_chunk[:context_length]\n",
    "\n",
    "            # tensor -> multi dimensional list\n",
    "            self.inputs.append(torch.tensor(input_chunk, dtype=torch.long)) # Embedding layer → torch.long ister\n",
    "            self.targets.append(torch.tensor(target_chunk, dtype=torch.long))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.targets[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec5d69f-d8fe-476e-9702-284533b855df",
   "metadata": {},
   "source": [
    "## 3 - TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c8a1c0e-0928-4ab3-b042-1a8bbeccd57b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[84,\n",
       " 161,\n",
       " 56,\n",
       " 161,\n",
       " 4,\n",
       " 161,\n",
       " 86,\n",
       " 161,\n",
       " 112,\n",
       " 161,\n",
       " 111,\n",
       " 161,\n",
       " 147,\n",
       " 161,\n",
       " 136,\n",
       " 161,\n",
       " 3,\n",
       " 161,\n",
       " 85,\n",
       " 161,\n",
       " 1,\n",
       " 161,\n",
       " 124,\n",
       " 161,\n",
       " 72,\n",
       " 0,\n",
       " 161,\n",
       " 134,\n",
       " 161,\n",
       " 78,\n",
       " 161,\n",
       " 82,\n",
       " 161,\n",
       " 159,\n",
       " 161,\n",
       " 104,\n",
       " 161,\n",
       " 160,\n",
       " 161,\n",
       " 32,\n",
       " 161,\n",
       " 139,\n",
       " 0,\n",
       " 161,\n",
       " 135,\n",
       " 161,\n",
       " 60,\n",
       " 161,\n",
       " 151,\n",
       " 161,\n",
       " 100,\n",
       " 161,\n",
       " 31,\n",
       " 161,\n",
       " 147,\n",
       " 161,\n",
       " 141,\n",
       " 161,\n",
       " 90,\n",
       " 161,\n",
       " 25,\n",
       " 0,\n",
       " 161,\n",
       " 134,\n",
       " 161,\n",
       " 66,\n",
       " 161,\n",
       " 80,\n",
       " 161,\n",
       " 19,\n",
       " 161,\n",
       " 120,\n",
       " 161,\n",
       " 117,\n",
       " 161,\n",
       " 53,\n",
       " 161,\n",
       " 129,\n",
       " 161,\n",
       " 75,\n",
       " 161,\n",
       " 61,\n",
       " 161,\n",
       " 62,\n",
       " 161,\n",
       " 71,\n",
       " 0,\n",
       " 161,\n",
       " 130,\n",
       " 161,\n",
       " 113,\n",
       " 161,\n",
       " 68,\n",
       " 161,\n",
       " 141,\n",
       " 161,\n",
       " 134,\n",
       " 161,\n",
       " 107,\n",
       " 161,\n",
       " 72,\n",
       " 0,\n",
       " 161,\n",
       " 11,\n",
       " 161,\n",
       " 106,\n",
       " 161,\n",
       " 143,\n",
       " 161,\n",
       " 48,\n",
       " 161,\n",
       " 134,\n",
       " 161,\n",
       " 109,\n",
       " 161,\n",
       " 30,\n",
       " 161,\n",
       " 141,\n",
       " 161,\n",
       " 15,\n",
       " 161,\n",
       " 32,\n",
       " 161,\n",
       " 149,\n",
       " 0,\n",
       " 161,\n",
       " 132,\n",
       " 161,\n",
       " 142,\n",
       " 161,\n",
       " 137,\n",
       " 161,\n",
       " 110,\n",
       " 161,\n",
       " 55,\n",
       " 161,\n",
       " 65,\n",
       " 161,\n",
       " 49,\n",
       " 0,\n",
       " 161,\n",
       " 6,\n",
       " 161,\n",
       " 73,\n",
       " 161,\n",
       " 76,\n",
       " 161,\n",
       " 17,\n",
       " 0,\n",
       " 161,\n",
       " 7,\n",
       " 161,\n",
       " 153,\n",
       " 161,\n",
       " 57,\n",
       " 161,\n",
       " 91,\n",
       " 161,\n",
       " 158,\n",
       " 161,\n",
       " 148,\n",
       " 0,\n",
       " 161,\n",
       " 21,\n",
       " 161,\n",
       " 157,\n",
       " 161,\n",
       " 7,\n",
       " 161,\n",
       " 154,\n",
       " 161,\n",
       " 70,\n",
       " 0,\n",
       " 161,\n",
       " 13,\n",
       " 161,\n",
       " 156,\n",
       " 161,\n",
       " 141,\n",
       " 161,\n",
       " 84,\n",
       " 161,\n",
       " 38,\n",
       " 161,\n",
       " 44,\n",
       " 161,\n",
       " 155,\n",
       " 161,\n",
       " 98,\n",
       " 161,\n",
       " 150,\n",
       " 0,\n",
       " 161,\n",
       " 29,\n",
       " 161,\n",
       " 121,\n",
       " 161,\n",
       " 116,\n",
       " 161,\n",
       " 114,\n",
       " 161,\n",
       " 61,\n",
       " 161,\n",
       " 123,\n",
       " 0,\n",
       " 161,\n",
       " 2,\n",
       " 161,\n",
       " 152,\n",
       " 161,\n",
       " 57,\n",
       " 161,\n",
       " 92,\n",
       " 161,\n",
       " 19,\n",
       " 161,\n",
       " 67,\n",
       " 161,\n",
       " 103,\n",
       " 161,\n",
       " 24,\n",
       " 0,\n",
       " 161,\n",
       " 133,\n",
       " 161,\n",
       " 22,\n",
       " 161,\n",
       " 102,\n",
       " 161,\n",
       " 86,\n",
       " 161,\n",
       " 138,\n",
       " 161,\n",
       " 4,\n",
       " 161,\n",
       " 97,\n",
       " 161,\n",
       " 33,\n",
       " 0,\n",
       " 161,\n",
       " 28,\n",
       " 161,\n",
       " 117,\n",
       " 161,\n",
       " 88,\n",
       " 161,\n",
       " 141,\n",
       " 161,\n",
       " 83,\n",
       " 161,\n",
       " 81,\n",
       " 161,\n",
       " 27,\n",
       " 161,\n",
       " 16,\n",
       " 161,\n",
       " 40,\n",
       " 161,\n",
       " 101,\n",
       " 161,\n",
       " 24,\n",
       " 0,\n",
       " 161,\n",
       " 6,\n",
       " 161,\n",
       " 137,\n",
       " 161,\n",
       " 5,\n",
       " 161,\n",
       " 50,\n",
       " 161,\n",
       " 141,\n",
       " 161,\n",
       " 69,\n",
       " 161,\n",
       " 79,\n",
       " 161,\n",
       " 52,\n",
       " 161,\n",
       " 23,\n",
       " 0,\n",
       " 161,\n",
       " 22,\n",
       " 161,\n",
       " 35,\n",
       " 161,\n",
       " 96,\n",
       " 161,\n",
       " 46,\n",
       " 161,\n",
       " 9,\n",
       " 0,\n",
       " 161,\n",
       " 133,\n",
       " 161,\n",
       " 42,\n",
       " 161,\n",
       " 127,\n",
       " 161,\n",
       " 99,\n",
       " 161,\n",
       " 12,\n",
       " 0,\n",
       " 161,\n",
       " 16,\n",
       " 161,\n",
       " 128,\n",
       " 161,\n",
       " 48,\n",
       " 161,\n",
       " 134,\n",
       " 161,\n",
       " 18,\n",
       " 161,\n",
       " 94,\n",
       " 161,\n",
       " 109,\n",
       " 161,\n",
       " 30,\n",
       " 0,\n",
       " 161,\n",
       " 16,\n",
       " 161,\n",
       " 8,\n",
       " 161,\n",
       " 126,\n",
       " 161,\n",
       " 19,\n",
       " 161,\n",
       " 10,\n",
       " 161,\n",
       " 59,\n",
       " 161,\n",
       " 158,\n",
       " 161,\n",
       " 34,\n",
       " 0,\n",
       " 161,\n",
       " 16,\n",
       " 161,\n",
       " 74,\n",
       " 161,\n",
       " 63,\n",
       " 161,\n",
       " 105,\n",
       " 161,\n",
       " 89,\n",
       " 161,\n",
       " 93,\n",
       " 161,\n",
       " 58,\n",
       " 161,\n",
       " 41,\n",
       " 0,\n",
       " 161,\n",
       " 108,\n",
       " 161,\n",
       " 119,\n",
       " 161,\n",
       " 73,\n",
       " 161,\n",
       " 19,\n",
       " 161,\n",
       " 118,\n",
       " 161,\n",
       " 131,\n",
       " 0,\n",
       " 161,\n",
       " 95,\n",
       " 161,\n",
       " 87,\n",
       " 161,\n",
       " 54,\n",
       " 161,\n",
       " 125,\n",
       " 161,\n",
       " 39,\n",
       " 161,\n",
       " 26,\n",
       " 161,\n",
       " 47,\n",
       " 161,\n",
       " 20,\n",
       " 161,\n",
       " 94,\n",
       " 161,\n",
       " 64,\n",
       " 161,\n",
       " 37,\n",
       " 0,\n",
       " 161,\n",
       " 86,\n",
       " 161,\n",
       " 140,\n",
       " 161,\n",
       " 54,\n",
       " 161,\n",
       " 51,\n",
       " 161,\n",
       " 77,\n",
       " 161,\n",
       " 145,\n",
       " 161,\n",
       " 36,\n",
       " 161,\n",
       " 146,\n",
       " 161,\n",
       " 14,\n",
       " 0,\n",
       " 161,\n",
       " 134,\n",
       " 161,\n",
       " 122,\n",
       " 161,\n",
       " 115,\n",
       " 161,\n",
       " 43,\n",
       " 161,\n",
       " 45,\n",
       " 161,\n",
       " 144,\n",
       " 0]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(vocab)\n",
    "tokens = tokenizer.encode(text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b020f89-e22b-4bc9-a281-010c3d0004f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MasterLLMModel(\n",
       "  (embedding): Embedding(\n",
       "    (embedding): Embedding(163, 32)\n",
       "  )\n",
       "  (layers): Sequential(\n",
       "    (0): DecoderBlock(\n",
       "      (self_attention): MultiHeadAttention(\n",
       "        (multi_head_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "        (projection): Linear(in_features=32, out_features=32, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (up_proj): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (down_proj): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (gelu): GELU()\n",
       "      )\n",
       "      (norm2): LayerNorm()\n",
       "    )\n",
       "    (1): DecoderBlock(\n",
       "      (self_attention): MultiHeadAttention(\n",
       "        (multi_head_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "        (projection): Linear(in_features=32, out_features=32, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (up_proj): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (down_proj): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (gelu): GELU()\n",
       "      )\n",
       "      (norm2): LayerNorm()\n",
       "    )\n",
       "    (2): DecoderBlock(\n",
       "      (self_attention): MultiHeadAttention(\n",
       "        (multi_head_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "        (projection): Linear(in_features=32, out_features=32, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (up_proj): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (down_proj): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (gelu): GELU()\n",
       "      )\n",
       "      (norm2): LayerNorm()\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=32, out_features=163, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim = 32\n",
    "context_length = 12\n",
    "vocab_size = len(tokenizer.vocab)\n",
    "\n",
    "torch.manual_seed(0)  # Reproducibility için rastgele tohum belirleme\n",
    "\n",
    "model = MasterLLMModel(vocab_size, embedding_dim, context_length, dropout_rate=0.5, num_heads=2, num_layers=3)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3fb52e53-ea34-4535-882b-0c4d09e8ddf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(205, 205)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stride = 2\n",
    "\n",
    "dataset = TextDataset(tokens, context_length, stride)\n",
    "len(dataset.inputs), len(dataset.targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dfbb40e5-4987-4750-9231-32422742b890",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 163])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out0 = model(dataset.inputs[0])\n",
    "out0.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8993cba7-8202-4a57-b116-9ed7ef68cd57",
   "metadata": {},
   "source": [
    "## LOSS FUNCTION - CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "99cec3f8-0d48-40be-b2e6-8ab2a6901bab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.7166, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "loss = loss_fn(out0, dataset.targets[0])\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e27602-384f-41bc-81cb-207323b3a854",
   "metadata": {},
   "source": [
    "## OPTIMIZER - AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "86f5349b-8c9c-40e3-b638-2a8e0ec923e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "80245c1b-f721-47b8-b3f1-f592ee276aa3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 2.6079323291778564 Average Loss: 2.832050931744459\n",
      "Epoch 2 Loss: 2.352301836013794 Average Loss: 2.479521927019445\n",
      "Epoch 3 Loss: 2.436436891555786 Average Loss: 2.281696034059292\n",
      "Epoch 4 Loss: 2.2793798446655273 Average Loss: 2.2057758057989725\n",
      "Epoch 5 Loss: 2.1747593879699707 Average Loss: 2.0757450167725726\n",
      "Epoch 6 Loss: 2.1898000240325928 Average Loss: 1.9558254108196351\n",
      "Epoch 7 Loss: 2.246631383895874 Average Loss: 1.8921991383157126\n",
      "Epoch 8 Loss: 2.191065788269043 Average Loss: 1.8473656508980727\n",
      "Epoch 9 Loss: 2.2283077239990234 Average Loss: 1.8500280630297776\n",
      "Epoch 10 Loss: 2.755199670791626 Average Loss: 1.8879931915097121\n",
      "Epoch 11 Loss: 2.2491819858551025 Average Loss: 1.9421470444376876\n",
      "Epoch 12 Loss: 2.7812461853027344 Average Loss: 1.9784919814365667\n",
      "Epoch 13 Loss: 2.6682260036468506 Average Loss: 2.245233061836987\n",
      "Epoch 14 Loss: 2.7451722621917725 Average Loss: 2.1970950894239474\n",
      "Epoch 15 Loss: 2.386631965637207 Average Loss: 2.2113191587168997\n",
      "Epoch 16 Loss: 2.586580991744995 Average Loss: 2.1177067372857072\n",
      "Epoch 17 Loss: 2.7827255725860596 Average Loss: 2.415455918195771\n",
      "Epoch 18 Loss: 2.4495441913604736 Average Loss: 2.2124896002978813\n",
      "Epoch 19 Loss: 2.551201820373535 Average Loss: 2.163191385385467\n",
      "Epoch 20 Loss: 2.4672226905822754 Average Loss: 2.265247686897836\n",
      "Epoch 21 Loss: 2.6749372482299805 Average Loss: 2.2863582866947825\n",
      "Epoch 22 Loss: 2.6119303703308105 Average Loss: 2.275938666739115\n",
      "Epoch 23 Loss: 2.732740640640259 Average Loss: 2.3752518165402297\n",
      "Epoch 24 Loss: 2.875518798828125 Average Loss: 2.378950629583219\n",
      "Epoch 25 Loss: 2.962442398071289 Average Loss: 2.3936762344546434\n",
      "Epoch 26 Loss: 2.8392741680145264 Average Loss: 2.397714632894935\n",
      "Epoch 27 Loss: 2.7713658809661865 Average Loss: 2.400695575737372\n",
      "Epoch 28 Loss: 2.758803367614746 Average Loss: 2.3885151328110115\n",
      "Epoch 29 Loss: 2.784215211868286 Average Loss: 2.3850699290996644\n",
      "Epoch 30 Loss: 2.739006280899048 Average Loss: 2.3772801463196918\n",
      "Epoch 31 Loss: 2.758223533630371 Average Loss: 2.3764983148109624\n",
      "Epoch 32 Loss: 2.7520291805267334 Average Loss: 2.3682935726351855\n",
      "Epoch 33 Loss: 2.7303621768951416 Average Loss: 2.3598823431061535\n",
      "Epoch 34 Loss: 2.7140514850616455 Average Loss: 2.3474004477989383\n",
      "Epoch 35 Loss: 2.7070112228393555 Average Loss: 2.338630610559045\n",
      "Epoch 36 Loss: 2.7235910892486572 Average Loss: 2.3371111782585703\n",
      "Epoch 37 Loss: 2.7346465587615967 Average Loss: 2.3249422782804907\n",
      "Epoch 38 Loss: 2.742558240890503 Average Loss: 2.321923421650398\n",
      "Epoch 39 Loss: 2.7418413162231445 Average Loss: 2.3219425375868634\n",
      "Epoch 40 Loss: 2.7421462535858154 Average Loss: 2.316132883909272\n",
      "Epoch 41 Loss: 2.738213539123535 Average Loss: 2.311948105765552\n",
      "Epoch 42 Loss: 2.7542479038238525 Average Loss: 2.3089565515518187\n",
      "Epoch 43 Loss: 2.7344706058502197 Average Loss: 2.3058953820205317\n",
      "Epoch 44 Loss: 2.7397022247314453 Average Loss: 2.297544632888422\n",
      "Epoch 45 Loss: 2.8863391876220703 Average Loss: 2.307292205531423\n",
      "Epoch 46 Loss: 2.767622232437134 Average Loss: 2.3019372422520705\n",
      "Epoch 47 Loss: 2.743389129638672 Average Loss: 2.3041451151778056\n",
      "Epoch 48 Loss: 2.596184730529785 Average Loss: 2.3304013263888477\n",
      "Epoch 49 Loss: 2.5528757572174072 Average Loss: 2.3046409665084466\n",
      "Epoch 50 Loss: 2.4936115741729736 Average Loss: 2.3098037091697137\n",
      "Epoch 51 Loss: 2.6080799102783203 Average Loss: 2.293893859444595\n",
      "Epoch 52 Loss: 2.5830323696136475 Average Loss: 2.3124916035954546\n",
      "Epoch 53 Loss: 2.5896623134613037 Average Loss: 2.283889983921516\n",
      "Epoch 54 Loss: 2.5552260875701904 Average Loss: 2.2585835602225326\n",
      "Epoch 55 Loss: 2.4645633697509766 Average Loss: 2.240051220103008\n",
      "Epoch 56 Loss: 2.382664918899536 Average Loss: 2.2535794886147102\n",
      "Epoch 57 Loss: 2.913182497024536 Average Loss: 2.2666048968710553\n",
      "Epoch 58 Loss: 2.4257915019989014 Average Loss: 2.226882882234527\n",
      "Epoch 59 Loss: 2.6541669368743896 Average Loss: 2.205004842106889\n",
      "Epoch 60 Loss: 2.522392988204956 Average Loss: 2.1878257361853994\n",
      "Epoch 61 Loss: 2.3297243118286133 Average Loss: 2.153048255385422\n",
      "Epoch 62 Loss: 2.330040454864502 Average Loss: 2.132383340161021\n",
      "Epoch 63 Loss: 2.461810350418091 Average Loss: 2.11848240189436\n",
      "Epoch 64 Loss: 2.694546937942505 Average Loss: 2.191577117617537\n",
      "Epoch 65 Loss: 2.569629669189453 Average Loss: 2.131642935915691\n",
      "Epoch 66 Loss: 2.4595425128936768 Average Loss: 2.1114161450688433\n",
      "Epoch 67 Loss: 2.4653711318969727 Average Loss: 2.093589283780354\n",
      "Epoch 68 Loss: 2.4114835262298584 Average Loss: 2.0723346431080887\n",
      "Epoch 69 Loss: 2.441401720046997 Average Loss: 2.0945069958524005\n",
      "Epoch 70 Loss: 2.2868430614471436 Average Loss: 2.058604495118304\n",
      "Epoch 71 Loss: 2.494002103805542 Average Loss: 2.0402061956684765\n",
      "Epoch 72 Loss: 2.2220399379730225 Average Loss: 2.032582041693897\n",
      "Epoch 73 Loss: 2.1541244983673096 Average Loss: 2.0331845626598453\n",
      "Epoch 74 Loss: 2.2215216159820557 Average Loss: 1.996102181876578\n",
      "Epoch 75 Loss: 2.1672046184539795 Average Loss: 1.9802890661286146\n",
      "Epoch 76 Loss: 2.238805055618286 Average Loss: 1.9555310400520882\n",
      "Epoch 77 Loss: 2.1380844116210938 Average Loss: 1.940936096121625\n",
      "Epoch 78 Loss: 2.3050801753997803 Average Loss: 2.006948182059497\n",
      "Epoch 79 Loss: 2.314563512802124 Average Loss: 1.993664778732672\n",
      "Epoch 80 Loss: 2.374736785888672 Average Loss: 1.9617284786410447\n",
      "Epoch 81 Loss: 2.3525948524475098 Average Loss: 2.0443911046516603\n",
      "Epoch 82 Loss: 2.255552053451538 Average Loss: 1.9671153684941733\n",
      "Epoch 83 Loss: 2.2454307079315186 Average Loss: 1.9446734655194167\n",
      "Epoch 84 Loss: 2.1404311656951904 Average Loss: 1.876398709343701\n",
      "Epoch 85 Loss: 2.3717563152313232 Average Loss: 1.8901995280893837\n",
      "Epoch 86 Loss: 2.0825388431549072 Average Loss: 1.8900538328217298\n",
      "Epoch 87 Loss: 2.002060651779175 Average Loss: 1.8715650163045743\n",
      "Epoch 88 Loss: 2.074004650115967 Average Loss: 1.851331254331077\n",
      "Epoch 89 Loss: 2.190467596054077 Average Loss: 1.826344531919898\n",
      "Epoch 90 Loss: 2.530071258544922 Average Loss: 1.8300605140081265\n",
      "Epoch 91 Loss: 2.171285629272461 Average Loss: 1.8404353409278684\n",
      "Epoch 92 Loss: 1.8421764373779297 Average Loss: 1.795198220741458\n",
      "Epoch 93 Loss: 2.88089919090271 Average Loss: 1.8336067217152292\n",
      "Epoch 94 Loss: 2.321556806564331 Average Loss: 1.8231571895320242\n",
      "Epoch 95 Loss: 1.9397072792053223 Average Loss: 1.7958580511372264\n",
      "Epoch 96 Loss: 2.060197114944458 Average Loss: 1.8250890464317508\n",
      "Epoch 97 Loss: 2.098176956176758 Average Loss: 1.7845308414319667\n",
      "Epoch 98 Loss: 2.011399507522583 Average Loss: 1.8164196880852304\n",
      "Epoch 99 Loss: 1.8943146467208862 Average Loss: 1.6910473678170181\n",
      "Epoch 100 Loss: 2.1192376613616943 Average Loss: 1.7237414947370204\n",
      "Epoch 101 Loss: 1.8480204343795776 Average Loss: 1.6698278659727515\n",
      "Epoch 102 Loss: 1.7961503267288208 Average Loss: 1.6611663952106384\n",
      "Epoch 103 Loss: 1.851091980934143 Average Loss: 1.7345361953828393\n",
      "Epoch 104 Loss: 1.7866593599319458 Average Loss: 1.7118090867996216\n",
      "Epoch 105 Loss: 1.815816879272461 Average Loss: 1.636996424488905\n",
      "Epoch 106 Loss: 1.7848916053771973 Average Loss: 1.6567230105400086\n",
      "Epoch 107 Loss: 1.7812986373901367 Average Loss: 1.621880699367058\n",
      "Epoch 108 Loss: 1.8734148740768433 Average Loss: 1.650626996086865\n",
      "Epoch 109 Loss: 1.705722451210022 Average Loss: 1.6128596832112567\n",
      "Epoch 110 Loss: 1.835655689239502 Average Loss: 1.6992682858211239\n",
      "Epoch 111 Loss: 1.8635135889053345 Average Loss: 1.6144120664131352\n",
      "Epoch 112 Loss: 1.764736294746399 Average Loss: 1.5652788295978453\n",
      "Epoch 113 Loss: 1.6439088582992554 Average Loss: 1.5573305036963485\n",
      "Epoch 114 Loss: 1.761142611503601 Average Loss: 1.6027006567978277\n",
      "Epoch 115 Loss: 1.6085282564163208 Average Loss: 1.571359198267867\n",
      "Epoch 116 Loss: 1.5837286710739136 Average Loss: 1.5145866161439476\n",
      "Epoch 117 Loss: 1.5275071859359741 Average Loss: 1.5494032438208418\n",
      "Epoch 118 Loss: 1.6118535995483398 Average Loss: 1.6506740866637812\n",
      "Epoch 119 Loss: 1.7144662141799927 Average Loss: 1.5389997610231725\n",
      "Epoch 120 Loss: 1.584190845489502 Average Loss: 1.4834432692062565\n",
      "Epoch 121 Loss: 1.59105384349823 Average Loss: 1.493776442655703\n",
      "Epoch 122 Loss: 1.7504371404647827 Average Loss: 1.448327753892759\n",
      "Epoch 123 Loss: 1.67067289352417 Average Loss: 1.492727101139906\n",
      "Epoch 124 Loss: 1.4202080965042114 Average Loss: 1.471786155061024\n",
      "Epoch 125 Loss: 1.652306079864502 Average Loss: 1.45424156799549\n",
      "Epoch 126 Loss: 1.3446236848831177 Average Loss: 1.4426264097050923\n",
      "Epoch 127 Loss: 1.5852187871932983 Average Loss: 1.4549393162494753\n",
      "Epoch 128 Loss: 1.7245875597000122 Average Loss: 1.438348675646433\n",
      "Epoch 129 Loss: 1.4754635095596313 Average Loss: 1.4598105939423165\n",
      "Epoch 130 Loss: 1.4861702919006348 Average Loss: 1.4497837717940167\n",
      "Epoch 131 Loss: 1.6140046119689941 Average Loss: 1.4163663579196464\n",
      "Epoch 132 Loss: 1.5883973836898804 Average Loss: 1.5041738606080777\n",
      "Epoch 133 Loss: 1.4231510162353516 Average Loss: 1.4470804752373114\n",
      "Epoch 134 Loss: 1.5645813941955566 Average Loss: 1.3854931784839164\n",
      "Epoch 135 Loss: 1.3114317655563354 Average Loss: 1.361249159603584\n",
      "Epoch 136 Loss: 1.5488156080245972 Average Loss: 1.3455146115000656\n",
      "Epoch 137 Loss: 1.2143722772598267 Average Loss: 1.3704965885092573\n",
      "Epoch 138 Loss: 1.2906277179718018 Average Loss: 1.3694333608557538\n",
      "Epoch 139 Loss: 1.2486261129379272 Average Loss: 1.3532549296937337\n",
      "Epoch 140 Loss: 1.6366658210754395 Average Loss: 1.4263460325031745\n",
      "Epoch 141 Loss: 1.6077440977096558 Average Loss: 1.3624089188692046\n",
      "Epoch 142 Loss: 1.354722023010254 Average Loss: 1.4586363158574918\n",
      "Epoch 143 Loss: 1.4088345766067505 Average Loss: 1.3626022103356152\n",
      "Epoch 144 Loss: 1.4056333303451538 Average Loss: 1.3257604607721654\n",
      "Epoch 145 Loss: 1.1210988759994507 Average Loss: 1.318506921791449\n",
      "Epoch 146 Loss: 1.5705679655075073 Average Loss: 1.3349721164238162\n",
      "Epoch 147 Loss: 1.3314965963363647 Average Loss: 1.3498877821899042\n",
      "Epoch 148 Loss: 1.3982443809509277 Average Loss: 1.4450619575453967\n",
      "Epoch 149 Loss: 1.376488208770752 Average Loss: 1.3098986076145638\n",
      "Epoch 150 Loss: 1.118331789970398 Average Loss: 1.2505061986969739\n",
      "Epoch 151 Loss: 1.3589626550674438 Average Loss: 1.285338465760394\n",
      "Epoch 152 Loss: 1.0942000150680542 Average Loss: 1.286017921785029\n",
      "Epoch 153 Loss: 1.204230546951294 Average Loss: 1.3079688415294741\n",
      "Epoch 154 Loss: 1.2753926515579224 Average Loss: 1.3041298642391113\n",
      "Epoch 155 Loss: 1.644597053527832 Average Loss: 1.3272312734185197\n",
      "Epoch 156 Loss: 0.9846007227897644 Average Loss: 1.252700701574\n",
      "Epoch 157 Loss: 1.4562653303146362 Average Loss: 1.2364514458470228\n",
      "Epoch 158 Loss: 0.9922065734863281 Average Loss: 1.2186791760165516\n",
      "Epoch 159 Loss: 1.1059199571609497 Average Loss: 1.2795334400200262\n",
      "Epoch 160 Loss: 1.1697221994400024 Average Loss: 1.219125153087988\n",
      "Epoch 161 Loss: 1.2743326425552368 Average Loss: 1.2103373524619312\n",
      "Epoch 162 Loss: 1.415712833404541 Average Loss: 1.2249859455155163\n",
      "Epoch 163 Loss: 1.2970350980758667 Average Loss: 1.1778588702039021\n",
      "Epoch 164 Loss: 1.485539436340332 Average Loss: 1.152965800936629\n",
      "Epoch 165 Loss: 1.2064999341964722 Average Loss: 1.1312913973157\n",
      "Epoch 166 Loss: 1.1470962762832642 Average Loss: 1.2211341849187525\n",
      "Epoch 167 Loss: 1.0262961387634277 Average Loss: 1.1823125547025262\n",
      "Epoch 168 Loss: 1.2588682174682617 Average Loss: 1.172399648951321\n",
      "Epoch 169 Loss: 1.5985721349716187 Average Loss: 1.1226366868833215\n",
      "Epoch 170 Loss: 1.1596928834915161 Average Loss: 1.0668882560439226\n",
      "Epoch 171 Loss: 0.8170275688171387 Average Loss: 1.0845752303193255\n",
      "Epoch 172 Loss: 1.034753680229187 Average Loss: 1.1333648938958238\n",
      "Epoch 173 Loss: 1.233574628829956 Average Loss: 1.0982486611459312\n",
      "Epoch 174 Loss: 0.9328634738922119 Average Loss: 1.0856764528809524\n",
      "Epoch 175 Loss: 0.7466211318969727 Average Loss: 1.0801137610179623\n",
      "Epoch 176 Loss: 1.1611850261688232 Average Loss: 1.0285741961583859\n",
      "Epoch 177 Loss: 0.9643128514289856 Average Loss: 1.0270845865331044\n",
      "Epoch 178 Loss: 1.1579569578170776 Average Loss: 1.0045428627874793\n",
      "Epoch 179 Loss: 1.4547699689865112 Average Loss: 1.063584520613275\n",
      "Epoch 180 Loss: 0.9043159484863281 Average Loss: 1.0725252980139197\n",
      "Epoch 181 Loss: 0.7344830632209778 Average Loss: 1.1063211885894217\n",
      "Epoch 182 Loss: 1.3748403787612915 Average Loss: 1.098088435719653\n",
      "Epoch 183 Loss: 1.2081754207611084 Average Loss: 1.056383865199438\n",
      "Epoch 184 Loss: 0.9672532081604004 Average Loss: 1.1416800235829703\n",
      "Epoch 185 Loss: 0.6641775965690613 Average Loss: 1.0524393977188482\n",
      "Epoch 186 Loss: 0.681240975856781 Average Loss: 1.010056349998567\n",
      "Epoch 187 Loss: 0.8380417823791504 Average Loss: 1.1000066828436967\n",
      "Epoch 188 Loss: 0.9895728230476379 Average Loss: 1.0120265364646912\n",
      "Epoch 189 Loss: 0.7262794375419617 Average Loss: 0.9625346669336644\n",
      "Epoch 190 Loss: 0.8829579949378967 Average Loss: 0.9475316716403496\n",
      "Epoch 191 Loss: 0.6631956696510315 Average Loss: 0.91269884371176\n",
      "Epoch 192 Loss: 0.5830695033073425 Average Loss: 0.8935876196477471\n",
      "Epoch 193 Loss: 0.7708268761634827 Average Loss: 0.9527166699490897\n",
      "Epoch 194 Loss: 1.0968323945999146 Average Loss: 0.9173788464650875\n",
      "Epoch 195 Loss: 0.7810080051422119 Average Loss: 0.8984577772093982\n",
      "Epoch 196 Loss: 0.7911725044250488 Average Loss: 0.8935512518010489\n",
      "Epoch 197 Loss: 0.3965754210948944 Average Loss: 0.8987705249611925\n",
      "Epoch 198 Loss: 0.6792371273040771 Average Loss: 0.8739691767750717\n",
      "Epoch 199 Loss: 0.47130146622657776 Average Loss: 0.9329816603079075\n",
      "Epoch 200 Loss: 0.6533254981040955 Average Loss: 0.9497970682818715\n",
      "Epoch 201 Loss: 0.4624454975128174 Average Loss: 0.8643908356747976\n",
      "Epoch 202 Loss: 0.7164473533630371 Average Loss: 0.8977050356748627\n",
      "Epoch 203 Loss: 0.7760529518127441 Average Loss: 0.8989385744420494\n",
      "Epoch 204 Loss: 0.7364020943641663 Average Loss: 0.8602280283846506\n",
      "Epoch 205 Loss: 0.9573305249214172 Average Loss: 0.8202842122170984\n",
      "Epoch 206 Loss: 0.9260562062263489 Average Loss: 0.7913811763612235\n",
      "Epoch 207 Loss: 0.6103110909461975 Average Loss: 0.7740083070789895\n",
      "Epoch 208 Loss: 0.532029390335083 Average Loss: 0.77051777563444\n",
      "Epoch 209 Loss: 0.538889467716217 Average Loss: 0.7721981491984391\n",
      "Epoch 210 Loss: 0.829368531703949 Average Loss: 0.78759497636702\n",
      "Epoch 211 Loss: 0.7663989663124084 Average Loss: 0.807542827798099\n",
      "Epoch 212 Loss: 0.8687484264373779 Average Loss: 0.7966666653388884\n",
      "Epoch 213 Loss: 0.8966309428215027 Average Loss: 0.7446988002556126\n",
      "Epoch 214 Loss: 0.9245765209197998 Average Loss: 0.7498044503898156\n",
      "Epoch 215 Loss: 0.4964502155780792 Average Loss: 0.7632317775633277\n",
      "Epoch 216 Loss: 0.3997304141521454 Average Loss: 0.7708710546900587\n",
      "Epoch 217 Loss: 0.3862089216709137 Average Loss: 0.7711161290727011\n",
      "Epoch 218 Loss: 0.3609045743942261 Average Loss: 0.6973834034873219\n",
      "Epoch 219 Loss: 0.5466940999031067 Average Loss: 0.6713946761154547\n",
      "Epoch 220 Loss: 0.49231791496276855 Average Loss: 0.6932990276958885\n",
      "Epoch 221 Loss: 0.7056906223297119 Average Loss: 0.6986892620237862\n",
      "Epoch 222 Loss: 0.580284059047699 Average Loss: 0.7500223242655033\n",
      "Epoch 223 Loss: 0.7080962061882019 Average Loss: 0.7580464569533744\n",
      "Epoch 224 Loss: 0.34467265009880066 Average Loss: 0.7689816026425943\n",
      "Epoch 225 Loss: 0.4915046691894531 Average Loss: 0.7729725907488567\n",
      "Epoch 226 Loss: 0.6667072176933289 Average Loss: 0.6855656754679796\n",
      "Epoch 227 Loss: 0.982283353805542 Average Loss: 0.6688461785636297\n",
      "Epoch 228 Loss: 0.38471460342407227 Average Loss: 0.6237229199671164\n",
      "Epoch 229 Loss: 0.6423712372779846 Average Loss: 0.6569824111170885\n",
      "Epoch 230 Loss: 0.44995343685150146 Average Loss: 0.6288445346239137\n",
      "Epoch 231 Loss: 0.29709628224372864 Average Loss: 0.6571274837342704\n",
      "Epoch 232 Loss: 1.0374618768692017 Average Loss: 0.6736003598062004\n",
      "Epoch 233 Loss: 0.4974280297756195 Average Loss: 0.663051910225938\n",
      "Epoch 234 Loss: 0.4633810222148895 Average Loss: 0.6464899573384262\n",
      "Epoch 235 Loss: 0.5354709029197693 Average Loss: 0.6501188245488376\n",
      "Epoch 236 Loss: 0.6904458403587341 Average Loss: 0.6325579768273889\n",
      "Epoch 237 Loss: 0.3070700466632843 Average Loss: 0.5861268290659276\n",
      "Epoch 238 Loss: 0.2975659668445587 Average Loss: 0.6030452862745378\n",
      "Epoch 239 Loss: 0.32618197798728943 Average Loss: 0.6218854675932628\n",
      "Epoch 240 Loss: 0.3522587716579437 Average Loss: 0.6296582618864571\n",
      "Epoch 241 Loss: 1.232176661491394 Average Loss: 0.6713773461376749\n",
      "Epoch 242 Loss: 0.726569652557373 Average Loss: 0.6167031898004253\n",
      "Epoch 243 Loss: 0.24254381656646729 Average Loss: 0.5992632853548702\n",
      "Epoch 244 Loss: 0.40898194909095764 Average Loss: 0.5900392799115762\n",
      "Epoch 245 Loss: 0.30644169449806213 Average Loss: 0.5741295905374899\n",
      "Epoch 246 Loss: 0.46786776185035706 Average Loss: 0.588027928587867\n",
      "Epoch 247 Loss: 0.4057372510433197 Average Loss: 0.5944398111686474\n",
      "Epoch 248 Loss: 0.18838469684123993 Average Loss: 0.564315267888511\n",
      "Epoch 249 Loss: 0.4410000145435333 Average Loss: 0.5904301023337899\n",
      "Epoch 250 Loss: 0.36147594451904297 Average Loss: 0.5940808407417157\n",
      "Epoch 251 Loss: 0.29247406125068665 Average Loss: 0.6249292793797283\n",
      "Epoch 252 Loss: 0.2619059383869171 Average Loss: 0.5731682946769202\n",
      "Epoch 253 Loss: 0.518653154373169 Average Loss: 0.532999516987219\n",
      "Epoch 254 Loss: 0.3592229187488556 Average Loss: 0.5462200278189124\n",
      "Epoch 255 Loss: 0.19956369698047638 Average Loss: 0.5371901034581952\n",
      "Epoch 256 Loss: 0.30659952759742737 Average Loss: 0.519677094897119\n",
      "Epoch 257 Loss: 0.4027461111545563 Average Loss: 0.4987955498259242\n",
      "Epoch 258 Loss: 0.5543166995048523 Average Loss: 0.5124104305738356\n",
      "Epoch 259 Loss: 0.5329336524009705 Average Loss: 0.5360586888906432\n",
      "Epoch 260 Loss: 0.2858341336250305 Average Loss: 0.5571521938937466\n",
      "Epoch 261 Loss: 0.5602226257324219 Average Loss: 0.5546484209415389\n",
      "Epoch 262 Loss: 0.233800008893013 Average Loss: 0.5225571249316379\n",
      "Epoch 263 Loss: 0.32409754395484924 Average Loss: 0.5131949944467079\n",
      "Epoch 264 Loss: 0.32187801599502563 Average Loss: 0.5367054888388005\n",
      "Epoch 265 Loss: 0.8037530779838562 Average Loss: 0.5773937219526709\n",
      "Epoch 266 Loss: 0.35267993807792664 Average Loss: 0.5710150690340414\n",
      "Epoch 267 Loss: 0.36766520142555237 Average Loss: 0.5100331938121377\n",
      "Epoch 268 Loss: 0.6939552426338196 Average Loss: 0.46606330442719346\n",
      "Epoch 269 Loss: 0.3361137807369232 Average Loss: 0.4671520260049076\n",
      "Epoch 270 Loss: 0.18777291476726532 Average Loss: 0.45728789022782956\n",
      "Epoch 271 Loss: 0.22145016491413116 Average Loss: 0.4805440838017115\n",
      "Epoch 272 Loss: 0.8181867003440857 Average Loss: 0.5420045995130771\n",
      "Epoch 273 Loss: 0.2930890619754791 Average Loss: 0.5164384602046594\n",
      "Epoch 274 Loss: 0.34244975447654724 Average Loss: 0.5266572914108998\n",
      "Epoch 275 Loss: 1.076276421546936 Average Loss: 0.5213130860793881\n",
      "Epoch 276 Loss: 0.42258235812187195 Average Loss: 0.5377397382404746\n",
      "Epoch 277 Loss: 0.620689868927002 Average Loss: 0.474032516850204\n",
      "Epoch 278 Loss: 0.3505968153476715 Average Loss: 0.464855167669494\n",
      "Epoch 279 Loss: 0.42615950107574463 Average Loss: 0.44952454661450736\n",
      "Epoch 280 Loss: 1.1166914701461792 Average Loss: 0.503977893438281\n",
      "Epoch 281 Loss: 0.7644967436790466 Average Loss: 0.5101898895894609\n",
      "Epoch 282 Loss: 0.21040143072605133 Average Loss: 0.4458210806657628\n",
      "Epoch 283 Loss: 0.16471244394779205 Average Loss: 0.44438087820279887\n",
      "Epoch 284 Loss: 0.44090017676353455 Average Loss: 0.46011203233061765\n",
      "Epoch 285 Loss: 0.7998080253601074 Average Loss: 0.4498757548448516\n",
      "Epoch 286 Loss: 0.43659719824790955 Average Loss: 0.46437857652582776\n",
      "Epoch 287 Loss: 0.2835444211959839 Average Loss: 0.48670726716518403\n",
      "Epoch 288 Loss: 0.22644920647144318 Average Loss: 0.4817879303562932\n",
      "Epoch 289 Loss: 0.24718326330184937 Average Loss: 0.44004530848526374\n",
      "Epoch 290 Loss: 0.2834111154079437 Average Loss: 0.43608781033899724\n",
      "Epoch 291 Loss: 0.2155437022447586 Average Loss: 0.39177460452405416\n",
      "Epoch 292 Loss: 0.20110736787319183 Average Loss: 0.3800971662489379\n",
      "Epoch 293 Loss: 0.1467939168214798 Average Loss: 0.3768298776047986\n",
      "Epoch 294 Loss: 0.253420889377594 Average Loss: 0.40338600881216\n",
      "Epoch 295 Loss: 0.27921172976493835 Average Loss: 0.41079865076192995\n",
      "Epoch 296 Loss: 0.51100093126297 Average Loss: 0.46284656561002496\n",
      "Epoch 297 Loss: 0.6264148354530334 Average Loss: 0.4944061552242535\n",
      "Epoch 298 Loss: 0.16135995090007782 Average Loss: 0.502220988200932\n",
      "Epoch 299 Loss: 0.330722451210022 Average Loss: 0.47170803714089277\n",
      "Epoch 300 Loss: 0.5967008471488953 Average Loss: 0.4366563107545783\n",
      "Epoch 301 Loss: 0.39751777052879333 Average Loss: 0.39166849259196257\n",
      "Epoch 302 Loss: 0.4484594166278839 Average Loss: 0.37007581933242517\n",
      "Epoch 303 Loss: 0.2181519716978073 Average Loss: 0.3640805060180222\n",
      "Epoch 304 Loss: 0.2672594487667084 Average Loss: 0.3637561531873738\n",
      "Epoch 305 Loss: 0.23137535154819489 Average Loss: 0.36830826407162154\n",
      "Epoch 306 Loss: 0.4402717649936676 Average Loss: 0.38363183006280804\n",
      "Epoch 307 Loss: 0.1415240615606308 Average Loss: 0.4556202877948924\n",
      "Epoch 308 Loss: 0.7310826182365417 Average Loss: 0.46242084001622547\n",
      "Epoch 309 Loss: 0.43812263011932373 Average Loss: 0.535475437888285\n",
      "Epoch 310 Loss: 0.5159630179405212 Average Loss: 0.4437872310964073\n",
      "Epoch 311 Loss: 0.3360554873943329 Average Loss: 0.38764487832057765\n",
      "Epoch 312 Loss: 0.12841014564037323 Average Loss: 0.35243124213160537\n",
      "Epoch 313 Loss: 0.16227303445339203 Average Loss: 0.34426925291375415\n",
      "Epoch 314 Loss: 0.6408705711364746 Average Loss: 0.3345807571600123\n",
      "Epoch 315 Loss: 0.12121015042066574 Average Loss: 0.3636900104037145\n",
      "Epoch 316 Loss: 0.22073715925216675 Average Loss: 0.4094459476267419\n",
      "Epoch 317 Loss: 0.24892741441726685 Average Loss: 0.3701001978956345\n",
      "Epoch 318 Loss: 0.3245493173599243 Average Loss: 0.42737167224106265\n",
      "Epoch 319 Loss: 0.23868773877620697 Average Loss: 0.44383809682799547\n",
      "Epoch 320 Loss: 0.1346035748720169 Average Loss: 0.45388835505014513\n",
      "Epoch 321 Loss: 0.12180192023515701 Average Loss: 0.3866686467899055\n",
      "Epoch 322 Loss: 0.24947412312030792 Average Loss: 0.33249139900251135\n",
      "Epoch 323 Loss: 0.13685496151447296 Average Loss: 0.32044789482180663\n",
      "Epoch 324 Loss: 0.1905277520418167 Average Loss: 0.33179371484169146\n",
      "Epoch 325 Loss: 0.2037431001663208 Average Loss: 0.2947769711475547\n",
      "Epoch 326 Loss: 0.12969113886356354 Average Loss: 0.28651784945551945\n",
      "Epoch 327 Loss: 0.14200930297374725 Average Loss: 0.2754190650291559\n",
      "Epoch 328 Loss: 0.13837631046772003 Average Loss: 0.31466316815193107\n",
      "Epoch 329 Loss: 0.38859987258911133 Average Loss: 0.31419026940697575\n",
      "Epoch 330 Loss: 0.17252880334854126 Average Loss: 0.34632960817799335\n",
      "Epoch 331 Loss: 0.43721234798431396 Average Loss: 0.3627518027839137\n",
      "Epoch 332 Loss: 0.13016656041145325 Average Loss: 0.3622420162325952\n",
      "Epoch 333 Loss: 0.2830025255680084 Average Loss: 0.35300823271092846\n",
      "Epoch 334 Loss: 0.329630047082901 Average Loss: 0.3258213360862034\n",
      "Epoch 335 Loss: 0.11578735709190369 Average Loss: 0.31723716227201426\n",
      "Epoch 336 Loss: 0.1519065648317337 Average Loss: 0.3070944192387709\n",
      "Epoch 337 Loss: 0.1138463094830513 Average Loss: 0.3254036736197588\n",
      "Epoch 338 Loss: 0.1727129966020584 Average Loss: 0.3176271597968369\n",
      "Epoch 339 Loss: 0.13606062531471252 Average Loss: 0.3130991289710126\n",
      "Epoch 340 Loss: 0.7551388740539551 Average Loss: 0.3470494151933164\n",
      "Epoch 341 Loss: 0.15906649827957153 Average Loss: 0.3442862214202561\n",
      "Epoch 342 Loss: 0.23036910593509674 Average Loss: 0.3284230114119809\n",
      "Epoch 343 Loss: 0.19853927195072174 Average Loss: 0.37782316069777416\n",
      "Epoch 344 Loss: 0.21194815635681152 Average Loss: 0.37108738098929567\n",
      "Epoch 345 Loss: 0.19079673290252686 Average Loss: 0.31202361328391043\n",
      "Epoch 346 Loss: 0.19388873875141144 Average Loss: 0.25542218678335593\n",
      "Epoch 347 Loss: 0.10937368869781494 Average Loss: 0.25421134103543874\n",
      "Epoch 348 Loss: 0.13288412988185883 Average Loss: 0.25209635771901867\n",
      "Epoch 349 Loss: 0.23838110268115997 Average Loss: 0.30543612075469845\n",
      "Epoch 350 Loss: 0.09687928110361099 Average Loss: 0.3434757834345829\n",
      "Epoch 351 Loss: 0.20528417825698853 Average Loss: 0.36098832075552245\n",
      "Epoch 352 Loss: 0.2206331044435501 Average Loss: 0.33964621255674016\n",
      "Epoch 353 Loss: 0.13259927928447723 Average Loss: 0.27631704619199765\n",
      "Epoch 354 Loss: 0.0704302042722702 Average Loss: 0.2503831864493649\n",
      "Epoch 355 Loss: 0.16506583988666534 Average Loss: 0.24938084152413578\n",
      "Epoch 356 Loss: 0.18754400312900543 Average Loss: 0.250245098551599\n",
      "Epoch 357 Loss: 0.25747939944267273 Average Loss: 0.25251249169431084\n",
      "Epoch 358 Loss: 0.26958373188972473 Average Loss: 0.3063982289375328\n",
      "Epoch 359 Loss: 0.21228408813476562 Average Loss: 0.29674598519213313\n",
      "Epoch 360 Loss: 0.07126195728778839 Average Loss: 0.2763403993009067\n",
      "Epoch 361 Loss: 0.11159779876470566 Average Loss: 0.24510122377698015\n",
      "Epoch 362 Loss: 0.03998132422566414 Average Loss: 0.2473896322544755\n",
      "Epoch 363 Loss: 0.18916553258895874 Average Loss: 0.2629589104343478\n",
      "Epoch 364 Loss: 0.16258959472179413 Average Loss: 0.26734156942040455\n",
      "Epoch 365 Loss: 0.7080292105674744 Average Loss: 0.2800040020357545\n",
      "Epoch 366 Loss: 0.3708127737045288 Average Loss: 0.28879043265450294\n",
      "Epoch 367 Loss: 0.4136674404144287 Average Loss: 0.3241151748724827\n",
      "Epoch 368 Loss: 0.11132024973630905 Average Loss: 0.27827503777495244\n",
      "Epoch 369 Loss: 0.1424526572227478 Average Loss: 0.26312882681263655\n",
      "Epoch 370 Loss: 0.2543621361255646 Average Loss: 0.2398057658134437\n",
      "Epoch 371 Loss: 0.0906427726149559 Average Loss: 0.23581328455631326\n",
      "Epoch 372 Loss: 0.4602745473384857 Average Loss: 0.2695012939321559\n",
      "Epoch 373 Loss: 0.07945502549409866 Average Loss: 0.3225911128357416\n",
      "Epoch 374 Loss: 0.05465828254818916 Average Loss: 0.27483539821171177\n",
      "Epoch 375 Loss: 0.1012950912117958 Average Loss: 0.24260779899976603\n",
      "Epoch 376 Loss: 0.09700465947389603 Average Loss: 0.20559645901184256\n",
      "Epoch 377 Loss: 0.05766850337386131 Average Loss: 0.17955199165678606\n",
      "Epoch 378 Loss: 0.05017882212996483 Average Loss: 0.20790700899755082\n",
      "Epoch 379 Loss: 0.062255457043647766 Average Loss: 0.2527827235211323\n",
      "Epoch 380 Loss: 0.08713983744382858 Average Loss: 0.24941058540489616\n",
      "Epoch 381 Loss: 0.18258978426456451 Average Loss: 0.2007223003157755\n",
      "Epoch 382 Loss: 0.04734225198626518 Average Loss: 0.23436242489189638\n",
      "Epoch 383 Loss: 0.06948397308588028 Average Loss: 0.21529437467092422\n",
      "Epoch 384 Loss: 0.05223093926906586 Average Loss: 0.2003284421272394\n",
      "Epoch 385 Loss: 0.3276256322860718 Average Loss: 0.23424769896196157\n",
      "Epoch 386 Loss: 0.04686993360519409 Average Loss: 0.23394839936276762\n",
      "Epoch 387 Loss: 0.33129361271858215 Average Loss: 0.2615809077260698\n",
      "Epoch 388 Loss: 0.15168018639087677 Average Loss: 0.25407022363619836\n",
      "Epoch 389 Loss: 1.4578717947006226 Average Loss: 0.305594323493722\n",
      "Epoch 390 Loss: 0.09416493773460388 Average Loss: 0.2243762206286192\n",
      "Epoch 391 Loss: 0.04941505566239357 Average Loss: 0.1890858613499781\n",
      "Epoch 392 Loss: 0.07932259887456894 Average Loss: 0.17346300976578055\n",
      "Epoch 393 Loss: 0.07881567627191544 Average Loss: 0.16458854436329226\n",
      "Epoch 394 Loss: 0.022026702761650085 Average Loss: 0.1805429668006737\n",
      "Epoch 395 Loss: 0.39073196053504944 Average Loss: 0.18120348802426967\n",
      "Epoch 396 Loss: 0.8230469822883606 Average Loss: 0.21389272472389587\n",
      "Epoch 397 Loss: 0.46801257133483887 Average Loss: 0.20698419971982154\n",
      "Epoch 398 Loss: 0.09239690750837326 Average Loss: 0.19696754297105276\n",
      "Epoch 399 Loss: 0.09569096565246582 Average Loss: 0.25287648912337496\n",
      "Epoch 400 Loss: 0.028297729790210724 Average Loss: 0.2782339958973774\n",
      "Epoch 401 Loss: 0.06410606950521469 Average Loss: 0.2305102229209208\n",
      "Epoch 402 Loss: 0.027596520259976387 Average Loss: 0.16602600651179872\n",
      "Epoch 403 Loss: 0.13141430914402008 Average Loss: 0.18234107246985887\n",
      "Epoch 404 Loss: 0.051279887557029724 Average Loss: 0.20905645676411508\n",
      "Epoch 405 Loss: 0.05836145579814911 Average Loss: 0.1911092278797452\n",
      "Epoch 406 Loss: 0.11874497681856155 Average Loss: 0.18164922535510325\n",
      "Epoch 407 Loss: 0.06288424879312515 Average Loss: 0.17429063564802452\n",
      "Epoch 408 Loss: 0.05827077105641365 Average Loss: 0.16804356036208024\n",
      "Epoch 409 Loss: 0.15793977677822113 Average Loss: 0.20227357673390609\n",
      "Epoch 410 Loss: 0.17796318233013153 Average Loss: 0.23796616138481513\n",
      "Epoch 411 Loss: 0.23808114230632782 Average Loss: 0.1972431257457995\n",
      "Epoch 412 Loss: 0.21659629046916962 Average Loss: 0.1752219223712639\n",
      "Epoch 413 Loss: 0.278250128030777 Average Loss: 0.17461856318319716\n",
      "Epoch 414 Loss: 0.02343790978193283 Average Loss: 0.18183022474461213\n",
      "Epoch 415 Loss: 0.020092245191335678 Average Loss: 0.16711551331256222\n",
      "Epoch 416 Loss: 0.07914746552705765 Average Loss: 0.17206617186481996\n",
      "Epoch 417 Loss: 0.05074186250567436 Average Loss: 0.22852480373822334\n",
      "Epoch 418 Loss: 0.2389758825302124 Average Loss: 0.2617032251388925\n",
      "Epoch 419 Loss: 0.03556446358561516 Average Loss: 0.21189952737129317\n",
      "Epoch 420 Loss: 0.031752463430166245 Average Loss: 0.21058818565272702\n",
      "Epoch 421 Loss: 0.021483326330780983 Average Loss: 0.12605097684100633\n",
      "Epoch 422 Loss: 0.0441083163022995 Average Loss: 0.1105563927260114\n",
      "Epoch 423 Loss: 0.04287818446755409 Average Loss: 0.11199448918469432\n",
      "Epoch 424 Loss: 0.024567222222685814 Average Loss: 0.12825620029030776\n",
      "Epoch 425 Loss: 0.04495343565940857 Average Loss: 0.1487860788931934\n",
      "Epoch 426 Loss: 0.18208365142345428 Average Loss: 0.1736159979979076\n",
      "Epoch 427 Loss: 0.3598475754261017 Average Loss: 0.16706307559478573\n",
      "Epoch 428 Loss: 0.041765276342630386 Average Loss: 0.1726437721537744\n",
      "Epoch 429 Loss: 0.013363261707127094 Average Loss: 0.1854926030128831\n",
      "Epoch 430 Loss: 0.03305836394429207 Average Loss: 0.16509319125424798\n",
      "Epoch 431 Loss: 0.2421027272939682 Average Loss: 0.16307380026342666\n",
      "Epoch 432 Loss: 0.12347817420959473 Average Loss: 0.17955979660789415\n",
      "Epoch 433 Loss: 0.03942415118217468 Average Loss: 0.246209969094432\n",
      "Epoch 434 Loss: 0.2058982402086258 Average Loss: 0.19719429657135795\n",
      "Epoch 435 Loss: 0.46792206168174744 Average Loss: 0.18513565181595523\n",
      "Epoch 436 Loss: 0.07086151838302612 Average Loss: 0.15612899275587463\n",
      "Epoch 437 Loss: 0.0906715989112854 Average Loss: 0.11558766311443433\n",
      "Epoch 438 Loss: 0.10631071776151657 Average Loss: 0.11528331049059222\n",
      "Epoch 439 Loss: 0.024191811680793762 Average Loss: 0.11454560104076092\n",
      "Epoch 440 Loss: 0.023213157430291176 Average Loss: 0.11081365581556428\n",
      "Epoch 441 Loss: 0.025125985965132713 Average Loss: 0.11219445294132684\n",
      "Epoch 442 Loss: 0.04059680178761482 Average Loss: 0.08777074981594413\n",
      "Epoch 443 Loss: 0.03612610325217247 Average Loss: 0.10471930875692789\n",
      "Epoch 444 Loss: 0.03657064586877823 Average Loss: 0.13078699112847084\n",
      "Epoch 445 Loss: 0.04385414347052574 Average Loss: 0.1364275709657771\n",
      "Epoch 446 Loss: 0.06214356794953346 Average Loss: 0.14810490242500857\n",
      "Epoch 447 Loss: 0.05545256659388542 Average Loss: 0.14961165712646596\n",
      "Epoch 448 Loss: 0.03169874846935272 Average Loss: 0.12601483469011215\n",
      "Epoch 449 Loss: 0.03712711110711098 Average Loss: 0.15243367173754405\n",
      "Epoch 450 Loss: 0.20173247158527374 Average Loss: 0.1688196732696691\n",
      "Epoch 451 Loss: 0.2569270431995392 Average Loss: 0.1762982601527034\n",
      "Epoch 452 Loss: 0.05051379278302193 Average Loss: 0.15337438605862055\n",
      "Epoch 453 Loss: 0.045083511620759964 Average Loss: 0.1215273621321724\n",
      "Epoch 454 Loss: 0.028505148366093636 Average Loss: 0.14966725602109984\n",
      "Epoch 455 Loss: 0.2809814512729645 Average Loss: 0.17221785919490928\n",
      "Epoch 456 Loss: 0.04504052922129631 Average Loss: 0.19747593619565412\n",
      "Epoch 457 Loss: 0.021598486229777336 Average Loss: 0.2297017469684162\n",
      "Epoch 458 Loss: 0.14715540409088135 Average Loss: 0.2142289458387872\n",
      "Epoch 459 Loss: 0.13940739631652832 Average Loss: 0.1782501516295824\n",
      "Epoch 460 Loss: 0.046081576496362686 Average Loss: 0.15518383945497435\n",
      "Epoch 461 Loss: 0.017363328486680984 Average Loss: 0.12101098153649306\n",
      "Epoch 462 Loss: 0.028830381110310555 Average Loss: 0.09433879935886802\n",
      "Epoch 463 Loss: 0.025838496163487434 Average Loss: 0.0709848428587997\n",
      "Epoch 464 Loss: 0.02088727243244648 Average Loss: 0.07827569377472306\n",
      "Epoch 465 Loss: 0.014554835855960846 Average Loss: 0.09087410948420989\n",
      "Epoch 466 Loss: 0.1270180642604828 Average Loss: 0.1170788892544806\n",
      "Epoch 467 Loss: 0.0178737323731184 Average Loss: 0.13376361868321532\n",
      "Epoch 468 Loss: 0.1790064126253128 Average Loss: 0.224152794503039\n",
      "Epoch 469 Loss: 0.07579382508993149 Average Loss: 0.18292900031977674\n",
      "Epoch 470 Loss: 0.03374703601002693 Average Loss: 0.15745799198928404\n",
      "Epoch 471 Loss: 0.0875006690621376 Average Loss: 0.10689810884934736\n",
      "Epoch 472 Loss: 0.17340926826000214 Average Loss: 0.07725557022977893\n",
      "Epoch 473 Loss: 0.012827184051275253 Average Loss: 0.06700811064007078\n",
      "Epoch 474 Loss: 0.010909556411206722 Average Loss: 0.08097205676206547\n",
      "Epoch 475 Loss: 0.008308155462145805 Average Loss: 0.08175451890565455\n",
      "Epoch 476 Loss: 0.019610701128840446 Average Loss: 0.09311994807567538\n",
      "Epoch 477 Loss: 0.028746075928211212 Average Loss: 0.08971072954648152\n",
      "Epoch 478 Loss: 0.43454647064208984 Average Loss: 0.1027586866556326\n",
      "Epoch 479 Loss: 0.33182403445243835 Average Loss: 0.1401187588642465\n",
      "Epoch 480 Loss: 0.03684863820672035 Average Loss: 0.1257364933174558\n",
      "Epoch 481 Loss: 0.48914042115211487 Average Loss: 0.14273500683967297\n",
      "Epoch 482 Loss: 0.04884334281086922 Average Loss: 0.20496204455451267\n",
      "Epoch 483 Loss: 0.019766127690672874 Average Loss: 0.18840176464717198\n",
      "Epoch 484 Loss: 0.044364187866449356 Average Loss: 0.15868441293742963\n",
      "Epoch 485 Loss: 0.036943789571523666 Average Loss: 0.11140536803570462\n",
      "Epoch 486 Loss: 0.01593673601746559 Average Loss: 0.09395669631478264\n",
      "Epoch 487 Loss: 0.017636025324463844 Average Loss: 0.08625480102363792\n",
      "Epoch 488 Loss: 0.03187035396695137 Average Loss: 0.08752041817302021\n",
      "Epoch 489 Loss: 0.012780514545738697 Average Loss: 0.09771628583236257\n",
      "Epoch 490 Loss: 0.03236496075987816 Average Loss: 0.09468287037821804\n",
      "Epoch 491 Loss: 0.0802549198269844 Average Loss: 0.11828754357993602\n",
      "Epoch 492 Loss: 0.09219696372747421 Average Loss: 0.17270171751790658\n",
      "Epoch 493 Loss: 0.038150936365127563 Average Loss: 0.19059214841165556\n",
      "Epoch 494 Loss: 0.09850481152534485 Average Loss: 0.16397787853166823\n",
      "Epoch 495 Loss: 0.05280034616589546 Average Loss: 0.17432717873555859\n",
      "Epoch 496 Loss: 0.25110650062561035 Average Loss: 0.11171810107941671\n",
      "Epoch 497 Loss: 0.011733745224773884 Average Loss: 0.09275585505032412\n",
      "Epoch 498 Loss: 0.10089502483606339 Average Loss: 0.08295629398113616\n",
      "Epoch 499 Loss: 0.02403170056641102 Average Loss: 0.08406277743463472\n",
      "Epoch 500 Loss: 0.013155200518667698 Average Loss: 0.06379819273062777\n"
     ]
    }
   ],
   "source": [
    "epoch = 500\n",
    "\n",
    "for epoch in range(epoch):\n",
    "    total_loss = 0\n",
    "    for input_, target in dataset:\n",
    "        pred = model(input_)\n",
    "        loss = loss_fn(pred, target)\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "    avg_loss = total_loss / len(dataset)\n",
    "    print(f\"Epoch {epoch + 1} Loss: {loss.item()} Average Loss: {avg_loss}\") \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725b8708-74e6-4a91-9e4f-0b7d008fb7dc",
   "metadata": {},
   "source": [
    "## 4 - TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a0ce9879-1459-40b6-9f4b-1d6d23fcfc58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mh370 yolcu uçağı 8 mart 2014 tarihinde'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tokens = tokenizer.encode(\"mh370 yolcu\")\n",
    "out = model.generate(test_tokens,10)\n",
    "tokenizer.decode(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbd824d-1109-4569-9e6f-8be1b7b0a262",
   "metadata": {},
   "source": [
    "## SAVE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee5ea65-f837-4b13-85d0-7056b6478cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03488ac-428f-499a-abe1-7b73cb57ae3c",
   "metadata": {},
   "source": [
    "## LOAD MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5604316a-4a98-4642-b7c7-7f67c41cc9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = model = MasterLLMModel(vocab_size=len(vocab), embedding_dim=embedding_dim, context_length=context_length, dropout_rate=0.5, num_heads=2, num_layers=3)\n",
    "loaded_model.load_state_dict(torch.load(\"model.pth\"))\n",
    "loaded_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
